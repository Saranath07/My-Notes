[
  {
    "objectID": "pages/DL/IntroDL.html",
    "href": "pages/DL/IntroDL.html",
    "title": "Backpropogation",
    "section": "",
    "text": "Backpropogation is a technique which is used to find the gradients of each layer with respect to the loss. We can write our output function as:\n\\[\n\\hat{y}_i = \\hat{f}(x_i) = O(W_3 \\ g(W_2 \\ g(W_1 x_i + b_1) + b_2) + b_3)\n\\]\nThis is when we have 3 inputs, 2 hidden layers with 3 neurons in each and 1 output layer.\n\n\n\nExample Feed Forward Neural Network\n\n\nWe use the optimization algorithm Gradient Descent which can be used to traverse the loss function and get into the local minima.\nThe update rule goes like this:\n\\[\n\\theta_{t+1} = \\theta_{t} - \\eta \\nabla_{\\theta}\\mathcal{L}(\\theta)\n\\]\nwhere \\[\n\\begin{align*}\n\\theta  &  :     \\text{Parameters} \\\\\n\\eta & :  \\text{Learning Rate} \\\\\n\\nabla_{\\theta}\\mathcal{L}(\\theta) & : \\text{Gradient of loss function with respect to the parameters}.\n\\end{align*}\n\\]\nAs we have more parameters and functions, computation of gradient with respect to the parameters i.e.Â \\(\\nabla_{\\theta}\\mathcal{L}(\\theta)\\) is not straight forward.\nSo we depend on chain rule to get the solution.\nLet us try to compute just the loss with respect to \\(1\\) parameter.\n\\[\n\\dfrac{\\partial \\mathcal{L}(\\theta)}{\\partial W_{111}} = \\dfrac{\\partial \\mathcal{L}(\\theta)}{\\partial \\hat{y}}\\dfrac{\\partial \\hat{y}}{\\partial a_{L11}}\\dfrac{\\partial a_{L11}}{\\partial h_{21}}\\dfrac{\\partial h_{21}}{\\partial a_{21}}\\dfrac{\\partial a_{21}}{\\partial h_{11}}\\dfrac{\\partial h_{11}}{\\partial a_{11}}\\dfrac{\\partial a_{11}}{\\partial W_{111}}\n\\]\nBefore actually understanding this equation, let us see what each term refers to.\n\\[\n\\begin{align*}\n\\mathcal{L}(\\theta) & : \\text{Loss function with respect to } \\theta \\\\\n\\hat{y} & : \\text{Predicted } y \\\\\nh & : \\text{Activation layer} \\\\\na & : \\text{Pre-activation layer}\\\\\nW & : \\text{Weights} \\\\\nb & : \\text{Biases}\n\\end{align*}\n\\]\nFirstly, we have to forward propogate through layers. First of all we have to initialize \\(W_1\\).\n\n\n\nInput Layer to First Hidden Layer\n\n\nSimilarly we will do for other neurons.\n\n\n\nFirst Layer to Output Layer\n\n\nSo, now let us try to write the equations for them.\nFor the input layer to the hidden layer we have,\nFirst, we compute the values of pre-activation layer \\(a_i\\).\nTherefore, \\[\na_1 = W_1  \\textbf{x} + b_1\n\\] where \\(\\textbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n \\end{bmatrix}, \\textbf{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\\\ \\vdots \\\\ b_n \\end{bmatrix}\\) and \\(W_1 = \\begin{bmatrix} w_{11} & w_{12} & \\ldots & w_{1n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_{1n} & w_{2n} & \\ldots & w_{nn}  \\end{bmatrix}\\)\nHere we have \\(\\textbf{x} \\in \\mathbb{R}^n\\) and \\(W \\in \\mathbb{R}^{n \\times n}\\). (Note that all the layers have same number of neurons. If we have \\(m\\) neurons in hidden layer and \\(n\\) neurons in input layer, then we will have our weight matrix dimensions as \\(\\mathbb{R}^{m \\times n}\\))\nNow we have got values for the pre-activation layer. Now we have to intoduce the activation layer will be applied to every value in the pre-activation vector.\n\\[\nh_1 = g(a_1)\n\\]\nThen we will be performng this action for every layer.\nNow, as we have just 1 hidden layer in our example, we need to get the values for the output.\nNow we will use an output function and pass our hidden layer inputs to get the outputs. (This can potentially vary from the activation function.)\n\\[\n\\hat{y} = O(h_1)\n\\]\nNow let us try to take an example for \\(g\\) and \\(O\\) functions.\nIn this example we will use \\[\n\\begin{align*}\ng(x_i) & = \\dfrac{1}{1 + e^{-x_i}} \\\\\\\\\nO(x_i) & = \\dfrac{e^{x_i}}{\\sum_{j = 1}^{n}{x_j}}\n\\end{align*}\n\\] We usually call \\(g(x)\\) as sigmoid function adn \\(O(x)\\) as softmax function\nSo, we will use python to code them out.\n# Sigmoid Function\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Softmax Function\ndef softmax(L):\n    arr = []\n    for i in L:\n        arr.append(np.exp(i) / np.exp(L).sum())\n    return arr",
    "crumbs": [
      "Deep Learning",
      "Backpropogation"
    ]
  },
  {
    "objectID": "pages/MAB/IntroMAB.html",
    "href": "pages/MAB/IntroMAB.html",
    "title": "Introduction to Multi-Armed Bandits",
    "section": "",
    "text": "This page is for MAB"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Notes",
    "section": "",
    "text": "I put the course notes for my own easy perusal.\nprint(\"Hello World\")"
  },
  {
    "objectID": "pages/LLM/Week5.html",
    "href": "pages/LLM/Week5.html",
    "title": "Week 5 LLM",
    "section": "",
    "text": "Week 5 LLM\nWeek 5 of the course LLM"
  }
]