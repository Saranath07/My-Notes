[
  {
    "objectID": "pages/DL/IntroDL.html",
    "href": "pages/DL/IntroDL.html",
    "title": "Backpropogation",
    "section": "",
    "text": "Backpropogation is a technique which is used to find the gradients of each layer with respect to the loss. We can write our output function as:\n\\[\n\\hat{y}_i = \\hat{f}(x_i) = O(W_3 \\ g(W_2 \\ g(W_1 x_i + b_1) + b_2) + b_3)\n\\]\nThis is when we have 3 inputs, 2 hidden layers with 3 neurons in each and 1 output layer.\n\n\n\nExample Feed Forward Neural Network\n\n\nWe use the optimization algorithm Gradient Descent which can be used to traverse the loss function and get into the local minima.\nThe update rule goes like this:\n\\[\n\\theta_{t+1} = \\theta_{t} - \\eta \\nabla_{\\theta}\\mathcal{L}(\\theta)\n\\]\nwhere \\[\n\\begin{align*}\n\\theta  &  :     \\text{Parameters} \\\\\n\\eta & :  \\text{Learning Rate} \\\\\n\\nabla_{\\theta}\\mathcal{L}(\\theta) & : \\text{Gradient of loss function with respect to the parameters}.\n\\end{align*}\n\\]\nAs we have more parameters and functions, computation of gradient with respect to the parameters i.e.Â \\(\\nabla_{\\theta}\\mathcal{L}(\\theta)\\) is not straight forward.\nSo we depend on chain rule to get the solution.\nLet us try to compute just the loss with respect to \\(1\\) parameter.\n\\[\n\\dfrac{\\partial \\mathcal{L}(\\theta)}{\\partial W_{111}} = \\dfrac{\\partial \\mathcal{L}(\\theta)}{\\partial \\hat{y}}\\dfrac{\\partial \\hat{y}}{\\partial a_{L11}}\\dfrac{\\partial a_{L11}}{\\partial h_{21}}\\dfrac{\\partial h_{21}}{\\partial a_{21}}\\dfrac{\\partial a_{21}}{\\partial h_{11}}\\dfrac{\\partial h_{11}}{\\partial a_{11}}\\dfrac{\\partial a_{11}}{\\partial W_{111}}\n\\]\nBefore actually understanding this equation, let us see what each term refers to.\n\\[\n\\begin{align*}\n\\mathcal{L}(\\theta) & : \\text{Loss function with respect to } \\theta \\\\\n\\hat{y} & : \\text{Predicted } y \\\\\nh & : \\text{Activation layer} \\\\\na & : \\text{Pre-activation layer}\\\\\nW & : \\text{Weights} \\\\\nb & : \\text{Biases}\n\\end{align*}\n\\]\nNow let us understand how to backpropogate.",
    "crumbs": [
      "Deep Learning",
      "Backpropogation"
    ]
  },
  {
    "objectID": "pages/MAB/IntroMAB.html",
    "href": "pages/MAB/IntroMAB.html",
    "title": "Introduction to Multi-Armed Bandits",
    "section": "",
    "text": "This page is for MAB"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Notes",
    "section": "",
    "text": "I put the course notes for my own easy perusal.\nprint(\"Hello World\")"
  },
  {
    "objectID": "pages/LLM/Week5.html",
    "href": "pages/LLM/Week5.html",
    "title": "Week 5 LLM",
    "section": "",
    "text": "Week 5 LLM\nWeek 5 of the course LLM"
  }
]